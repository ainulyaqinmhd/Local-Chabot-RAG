{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO8ZGHIN0DIs/MrIoxaIj4K"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## RAG\n",
        "\n",
        "### Indexing\n",
        "1. **Load**: First we need to load our data. This is done with CSV Loaders\n",
        "2. **Split**: Text splitters break large Documents into smaller chunks. This is useful both for indexing data and for passing it in to a model, since large chunks are harder to search over and won't fit in a model's finite context window.\n",
        "3. **Store**: We need somewhere to store and index our splits, so that they can later be searched over. This is often done using a VectorStore and Embeddings model.\n",
        "\n",
        "### Retrieval and generation\n",
        "4. **Retrieve**: Given a user input, relevant splits are retrieved from storage using a Retriever.\n",
        "5. **Generate**: A ChatModel / LLM produces an answer using a prompt that includes the question and the retrieved data"
      ],
      "metadata": {
        "id": "_-uSwwvPfc7V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Package Installation"
      ],
      "metadata": {
        "id": "yL0ImVcY-x6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain_community langchain_chroma langchain-openai langchainhub gradio"
      ],
      "metadata": {
        "collapsed": true,
        "id": "vqW5m8Nje4sR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use OpenAI API Key"
      ],
      "metadata": {
        "id": "fnAiukCs-0n1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "# Set the OpenAI API key for accessing the OpenAI services\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Initialize the ChatOpenAI object with the GPT-4 model\n",
        "llm = ChatOpenAI(model=\"gpt-4o\")\n"
      ],
      "metadata": {
        "id": "8PQI2QNmexoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load data from CSV"
      ],
      "metadata": {
        "id": "bRRZsctV-3ua"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Pl1XM4peuDG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
        "\n",
        "# Define the directory containing the database files\n",
        "database_folder = \"Database\"\n",
        "\n",
        "# List all files in the database folder\n",
        "files = [os.path.join(database_folder, f) for f in os.listdir(database_folder) if os.path.isfile(os.path.join(database_folder, f))]\n",
        "\n",
        "all_docs = []\n",
        "\n",
        "# Function to detect the structure of a CSV file\n",
        "def detect_csv_structure(file_path):\n",
        "    with open(file_path, newline='', encoding='utf-8') as csvfile:\n",
        "        reader = csv.reader(csvfile)\n",
        "        header = next(reader)\n",
        "    return header\n",
        "\n",
        "# Load data from each CSV file in the folder\n",
        "for file in files:\n",
        "    if file.endswith(\".csv\"):\n",
        "        header = detect_csv_structure(file)\n",
        "        print(f\"Detected columns in {file}: {header}\")\n",
        "        source_column = \"URL\" if \"URL\" in header else header[0]  # Adjust source column as needed\n",
        "        loader = CSVLoader(\n",
        "            file_path=file,\n",
        "            source_column=source_column,\n",
        "            csv_args={\n",
        "                \"delimiter\": \",\",\n",
        "                \"quotechar\": '\"',\n",
        "                \"fieldnames\": header\n",
        "            }\n",
        "        )\n",
        "        docs = loader.load()\n",
        "        print(f\"Loaded {len(docs)} documents from {file}\")\n",
        "        all_docs.extend(docs)\n",
        "\n",
        "print(f\"Total documents loaded: {len(all_docs)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split data to chunks"
      ],
      "metadata": {
        "id": "JUijSxpb-7LO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Initialize the text splitter with specific chunk size and overlap\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
        ")\n",
        "all_splits = text_splitter.split_documents(all_docs)\n",
        "\n",
        "print(f\"Total splits created: {len(all_splits)}\")\n",
        "print(all_splits[0])\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "sbLJhuBofn0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Store document and embedding to vector database"
      ],
      "metadata": {
        "id": "TgGp4Uni--PC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_chroma import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "import time\n",
        "\n",
        "# Initialize the embeddings\n",
        "embedding = OpenAIEmbeddings(model='text-embedding-3-small')\n",
        "\n",
        "# Initialize the vector store for storing document embeddings\n",
        "vectorstore = Chroma(\n",
        "    collection_name=\"my_collection\",\n",
        "    embedding_function=embedding,\n",
        "    persist_directory=\"./chroma_db\"\n",
        ")\n",
        "\n",
        "batch_size = 10\n",
        "\n",
        "# Add document splits to the vector store in batches\n",
        "for i in range(0, len(all_splits), batch_size):\n",
        "    batch = all_splits[i: i + batch_size]\n",
        "    vectorstore.add_documents(batch)\n",
        "    time.sleep(1)\n"
      ],
      "metadata": {
        "id": "qtYO5Kcffqca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Similarity search"
      ],
      "metadata": {
        "id": "no6LXWyU_Nnt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import spatial\n",
        "\n",
        "# Testing question\n",
        "question = \"What is global investor expand engagement?\"\n",
        "\n",
        "# Create a retriever object to search for similar documents\n",
        "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
        "\n",
        "# Retrieve documents relevant to the question\n",
        "retrieved_docs = retriever.invoke(question)\n",
        "\n",
        "print(f\"Total documents retrieved: {len(retrieved_docs)}\")\n",
        "# print(retrieved_docs[0])\n",
        "\n",
        "# Calculate and print similarity scores\n",
        "question_embedding = embedding.embed_query(question)  # Embed the question\n",
        "for i in range(len(retrieved_docs)):\n",
        "    doc_embedding = embedding.embed_documents([retrieved_docs[i].page_content])[0]  # Embed the document\n",
        "    similarity = 1 - spatial.distance.cosine(question_embedding, doc_embedding)  # Calculate cosine similarity\n",
        "    print(f\"Document {i+1} (Similarity: {similarity:.4f}):\\n{retrieved_docs[i].page_content}\\n\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "rLnCNfjsfsx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reply generation"
      ],
      "metadata": {
        "id": "wmLsEXjv_QQ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import hub\n",
        "\n",
        "# Load a predefined prompt from the Langchain hub\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "# Generate example messages based on the retrieved documents and the question\n",
        "example_messages = prompt.invoke(\n",
        "    {\"context\": \"{retrieved_docs}\", \"question\": \"{your_question}\", \"reference\": \"{source}\"}\n",
        ").to_messages()\n",
        "\n",
        "print(example_messages[0].content)\n"
      ],
      "metadata": {
        "id": "igeIhNNxfwyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Customize the prompt"
      ],
      "metadata": {
        "id": "x9JmsHQ0_SU5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "# Define a custom prompt template\n",
        "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "Use three sentences maximum and keep the answer as concise as possible.\n",
        "Always say \"thanks for asking!\" at the end of the answer and followed by the reference contains URL source (if it doesn't contain URL don't put any reference).\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Helpful Answer:\n",
        "\n",
        "Reference:\n",
        "- {source_column}\"\"\"\n",
        "\n",
        "# Create a prompt object from the template\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "example_messages = prompt.invoke(\n",
        "    {\n",
        "        \"context\": retrieved_docs[0].page_content,\n",
        "        \"question\": question,\n",
        "        # \"source_file\": files[0],  # Replace with actual source file\n",
        "        \"source_column\": 'URL'  # Replace with actual source column\n",
        "    }\n",
        ").to_messages()\n",
        "\n",
        "print(example_messages[0].content)\n"
      ],
      "metadata": {
        "id": "KNndqitdfxiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LCEL (Langchain Expression Language)"
      ],
      "metadata": {
        "id": "qNSw_syw_VI9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# Function to format documents for the prompt\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "# Create a RAG (Retrieval-Augmented Generation) chain\n",
        "rag_chain = (\n",
        "    {\n",
        "        \"context\": retriever | format_docs,\n",
        "        \"question\": RunnablePassthrough(),\n",
        "        # \"source_file\": lambda x: files[0],  # Provide source_file\n",
        "        \"source_column\": lambda x: \"URL\"  # Provide source_column\n",
        "    }\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Stream the response from the RAG chain for the given question\n",
        "for chunk in rag_chain.stream(question):\n",
        "    print(chunk, end=\"\", flush=True)\n"
      ],
      "metadata": {
        "id": "50hXraezf0Do"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradio interface setup"
      ],
      "metadata": {
        "id": "egctzsDJ_a3t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import datetime\n",
        "import csv\n",
        "import os\n",
        "\n",
        "# Function to log interactions to a CSV file\n",
        "def log_interaction_csv(user_message, bot_message, vote_message=None, log_file=\"chat_log.csv\"):\n",
        "    file_exists = os.path.isfile(log_file)\n",
        "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "    try:\n",
        "        with open(log_file, \"a+\", newline=\"\", encoding=\"utf-8\") as file:\n",
        "            writer = csv.writer(file)\n",
        "            if not file_exists:\n",
        "                writer.writerow([\"Timestamp\", \"Query\", \"Answer\", \"Satisfaction\"])\n",
        "\n",
        "            if vote_message:\n",
        "                file.seek(0)\n",
        "                rows = list(csv.reader(file))\n",
        "                if len(rows) > 1:\n",
        "                    rows[-1][-1] = vote_message\n",
        "                    file.seek(0)\n",
        "                    file.truncate()\n",
        "                    writer.writerows(rows)\n",
        "            else:\n",
        "                writer.writerow([timestamp, user_message, bot_message, \"\"])\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error writing to file: {e}\")\n",
        "\n",
        "# Function to generate a chatbot response\n",
        "def chatbot_response(question):\n",
        "    response = \"\"\n",
        "    for chunk in rag_chain.stream(question):\n",
        "        response += chunk\n",
        "    return response\n",
        "\n",
        "# Function to handle user votes on responses\n",
        "def vote(data: gr.LikeData):\n",
        "    vote_message = \"Liked\" if data.liked else \"Disliked\"\n",
        "    log_interaction_csv(\"\", \"\", vote_message)\n",
        "    print(vote_message)\n",
        "\n",
        "# Gradio interface setup\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## TABC - ChatBot V.0.1\\nThe database of chatbot (V.0.1) now contains detailed information about the GRESB foundation, its impact, and sustainability focus.\")\n",
        "    chatbot = gr.Chatbot(label=\"Ask me anything!\")\n",
        "\n",
        "    with gr.Row():\n",
        "        txt = gr.Textbox(show_label=False, placeholder=\"Enter your question here...\")\n",
        "        submit_btn = gr.Button(\"Send\")\n",
        "        retry_btn = gr.Button(\"Regenerate\") # Add a retry button\n",
        "\n",
        "        # Function to handle user message input\n",
        "        def user_message(message, history):\n",
        "            history.append((message, None))\n",
        "            return history, \"\"\n",
        "\n",
        "        # Function to handle bot response\n",
        "        def bot_response(history):\n",
        "            user_message = history[-1][0]\n",
        "            bot_message = chatbot_response(user_message)\n",
        "            history[-1] = (user_message, bot_message)\n",
        "\n",
        "            # Log the interaction\n",
        "            log_interaction_csv(user_message, bot_message)\n",
        "\n",
        "            return history\n",
        "\n",
        "        # Function to handle retry\n",
        "        def retry(history):\n",
        "            if history:\n",
        "                last_question = history[-1][0]  # Get the last question\n",
        "                history.pop() # Remove the last interaction\n",
        "                # Re-run the last question by triggering user_message and bot_response\n",
        "                history, _ = user_message(last_question, history)\n",
        "                history = bot_response(history)\n",
        "            return history, \"\"\n",
        "\n",
        "    # Handle the submit, click, and retry events\n",
        "    txt.submit(user_message, [txt, chatbot], [chatbot, txt], queue=False).then(\n",
        "        bot_response, chatbot, chatbot\n",
        "    )\n",
        "    submit_btn.click(user_message, [txt, chatbot], [chatbot, txt], queue=False).then(\n",
        "        bot_response, chatbot, chatbot\n",
        "    )\n",
        "    retry_btn.click(retry, chatbot, [chatbot, txt])\n",
        "    # Add the voting functionality to the chatbot\n",
        "    chatbot.like(vote, None, None)\n",
        "\n",
        "# Launch the Gradio interface\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "id": "hlTdV8RSf2Pj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of examples\n",
        "\"\"\"\n",
        "1. What is global investor expand engagement?\n",
        "2. Give me some information about ticker named \"REG\"\n",
        "3. How did [Fund Name X] perform in the first quarter of 2020?\" (Replace [Fund Name X] with an actual fund from your data)\n",
        "4. Which fund in the [Region Y] region showed the strongest growth in 2021?\n",
        "5. What was the average performance of [Sector Z] funds in the second half of 2022?\n",
        "\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "1FMwVPKzxgfJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}